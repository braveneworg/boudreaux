name: CI/CD - Build, Publish, Deploy

# This workflow handles deployment to production
# Quality checks (tests, lint, typecheck) are performed by ci.yml
# This workflow only runs after ci.yml passes on the main branch

on:
  workflow_run:
    workflows: ["CI"]
    types:
      - completed
    branches: [main]

concurrency:
  group: cicd-${{ github.ref_name }}
  cancel-in-progress: true

env:
  REGISTRY: ghcr.io
  IMAGE_REPO: ghcr.io/${{ github.repository_owner }}/boudreaux
  WEBSITE_IMAGE: ghcr.io/${{ github.repository_owner }}/boudreaux/website
  NGINX_IMAGE: ghcr.io/${{ github.repository_owner }}/boudreaux/nginx

permissions:
  contents: read
  actions: read
  packages: write

jobs:
  download-build:
    name: Download build from CI
    runs-on: ubuntu-latest
    # Only run if CI workflow succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Check artifact presence in triggering CI run
        uses: actions/github-script@v7
        with:
          script: |
            const runId = context.payload.workflow_run?.id;
            if (!runId) {
              core.setFailed('No workflow_run.id found on event payload.');
              return;
            }
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${runId}`;
            core.info(`CI run URL: ${runUrl}`);
            const { data: artifacts } = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: runId,
              per_page: 100,
            });
            const names = artifacts.artifacts.map(a => a.name);
            core.info(`Found artifacts on CI run ${runId}: ${names.join(', ') || '(none)'}`);
            if (!names.includes('nextjs-build')) {
              core.setFailed("Missing required artifact 'nextjs-build' from CI run. Ensure ci.yml 'build' job uploaded it.");
            }

      - name: Download build artifact from triggering CI run
        uses: dawidd6/action-download-artifact@v6
        with:
          run_id: ${{ github.event.workflow_run.id }}
          name: nextjs-build
          path: .
          github_token: ${{ secrets.GITHUB_TOKEN }}

      - name: Verify artifact
        run: |
          if [ ! -f "next-build.tar.gz" ]; then
            echo "Error: next-build.tar.gz not found!"
            ls -la
            exit 1
          fi
          ls -lh next-build.tar.gz

      - name: Re-upload for downstream jobs
        uses: actions/upload-artifact@v4
        with:
          name: nextjs-build-deploy
          path: next-build.tar.gz
          retention-days: 1
          if-no-files-found: error

  sync-cdn:
    name: Sync static assets to CDN
    runs-on: ubuntu-latest
    needs: download-build
    outputs:
      CHUNKS_HASH: ${{ steps.verify-build.outputs.CHUNKS_HASH }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.12.0"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Download build artifact
        uses: actions/download-artifact@v4
        with:
          name: nextjs-build-deploy
          path: .

      - name: Extract build
        run: tar -xzf next-build.tar.gz

      - name: Verify build before sync
        id: verify-build
        run: |
          echo "=== Verifying Build Artifact Before CDN Sync ==="

          if [ ! -d ".next/static/chunks" ]; then
            echo "❌ ERROR: .next/static/chunks not found after extraction"
            exit 1
          fi

          # Show ALL chunk files that will be uploaded (not just sample)
          echo "Chunk files in build artifact:"
          ls -lh .next/static/chunks/*.js | awk '{print $9, $5}'

                    # Calculate checksum of entire build
          CHUNKS_HASH=$(find .next/static/chunks -name "*.js" -exec sha256sum {} \; | sha256sum | awk '{print $1}')
          echo "Build artifact checksum: $CHUNKS_HASH"
          echo "CHUNKS_HASH=$CHUNKS_HASH" >> $GITHUB_ENV
          echo "CHUNKS_HASH=$CHUNKS_HASH" >> $GITHUB_OUTPUT

          echo "✓ Build artifact verified and ready for CDN sync"

      - name: Verify CloudFront OAC configuration
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          CLOUDFRONT_DISTRIBUTION_ID: ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}
        run: |
          echo "Verifying CloudFront OAC is configured..."

          OAC_ID=$(aws cloudfront get-distribution \
            --id "$CLOUDFRONT_DISTRIBUTION_ID" \
            --query 'Distribution.DistributionConfig.Origins.Items[0].OriginAccessControlId' \
            --output text 2>/dev/null || echo "None")

          if [ "$OAC_ID" = "None" ] || [ -z "$OAC_ID" ]; then
            echo "⚠️  WARNING: CloudFront OAC not configured"
            echo "Run: scripts/manually-attach-oac.sh to configure it"
          else
            echo "✓ CloudFront OAC configured: $OAC_ID"
          fi

      - name: Sync to CDN
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
          CLOUDFRONT_DISTRIBUTION_ID: ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}
          CDN_DOMAIN: ${{ secrets.CDN_DOMAIN }}
        run: npm run sync:cdn:no-build

      - name: Verify files uploaded to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
        run: |
          echo "=== Verifying S3 Upload Completed Successfully ==="

          # Check file count
          FILE_COUNT=$(aws s3 ls "s3://$S3_BUCKET/media/_next/static/" --recursive | wc -l)

          if [ "$FILE_COUNT" -gt 0 ]; then
            echo "✓ $FILE_COUNT files in S3"
          else
            echo "❌ ERROR: No files found in S3"
            exit 1
          fi

          # Verify the webpack chunk matches what we just built
          LOCAL_WEBPACK=$(ls .next/static/chunks/webpack-*.js 2>/dev/null | head -1)
          if [ -n "$LOCAL_WEBPACK" ]; then
            WEBPACK_FILENAME=$(basename "$LOCAL_WEBPACK")
            S3_WEBPACK_KEY="media/_next/static/chunks/$WEBPACK_FILENAME"

            echo "Checking if webpack chunk was uploaded: $WEBPACK_FILENAME"
            if aws s3 ls "s3://$S3_BUCKET/$S3_WEBPACK_KEY" >/dev/null 2>&1; then
              echo "✓ Webpack chunk verified in S3: $WEBPACK_FILENAME"
            else
              echo "❌ ERROR: Webpack chunk NOT found in S3: $WEBPACK_FILENAME"
              echo "Files actually in S3:"
              aws s3 ls "s3://$S3_BUCKET/media/_next/static/chunks/" | grep webpack
              exit 1
            fi
          fi

          echo "✓ Build artifacts successfully synced to S3"

  build-images:
    name: Build & push Docker images
    runs-on: ubuntu-latest
    needs: download-build
    # Run in parallel with sync-cdn since they don't depend on each other
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download build artifact
        uses: actions/download-artifact@v4
        with:
          name: nextjs-build-deploy
          path: .

      - name: Verify and extract artifact
        run: |
          if [ ! -f "next-build.tar.gz" ]; then
            echo "Error: next-build.tar.gz not found after download!"
            ls -la
            exit 1
          fi
          echo "Artifact downloaded successfully:"
          ls -lh next-build.tar.gz

          # Extract to verify build matches sync-cdn
          echo "Extracting to verify build consistency..."
          tar -xzf next-build.tar.gz

          if [ -f ".next/BUILD_ID" ]; then
            BUILD_ID=$(cat .next/BUILD_ID)
            echo "BUILD_ID in Docker image build: $BUILD_ID"
            echo "DOCKER_BUILD_ID=$BUILD_ID" >> $GITHUB_ENV
          fi

          # Calculate checksum to match sync-cdn
          echo "Chunk files that will be in Docker image:"
          ls -lh .next/static/chunks/*.js | awk '{print $9, $5}'

          DOCKER_CHUNKS_HASH=$(find .next/static/chunks -name "*.js" -exec sha256sum {} \; | sha256sum | awk '{print $1}')
          echo "Docker build checksum: $DOCKER_CHUNKS_HASH"
          echo "DOCKER_CHUNKS_HASH=$DOCKER_CHUNKS_HASH" >> $GITHUB_ENV

          # Compare with sync-cdn checksum if available
          SYNC_HASH="${{ needs.sync-cdn.outputs.CHUNKS_HASH }}"
          if [ -n "$SYNC_HASH" ]; then
            echo "Comparing checksums between jobs:"
            echo "  sync-cdn:     $SYNC_HASH"
            echo "  build-images: $DOCKER_CHUNKS_HASH"

            if [ "$SYNC_HASH" != "$DOCKER_CHUNKS_HASH" ]; then
              echo "⚠️ WARNING: Build artifact mismatch detected!"
              echo "sync-cdn uploaded different files than will be in Docker images"
              echo "This will cause 403 errors on production!"
              exit 1
            fi
            echo "✓ Checksums match - build consistency verified"
          else
            echo "⚠️ sync-cdn checksum not available for comparison"
          fi

          # Clean up extraction (Dockerfile will extract fresh)
          rm -rf .next

          echo "✓ Build artifact verified and ready for Docker build"

      - name: Log in to container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.GHCR_USERNAME || github.actor }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Setup Docker layer caching
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          install: true
          driver-opts: |
            network=host
            image=moby/buildkit:latest
          buildkitd-flags: --allow-insecure-entitlement network.host --allow-insecure-entitlement security.insecure

      - name: Build website image (single-arch for speed)
        env:
          NEXT_PUBLIC_CLOUDFLARE_SITE_KEY: ${{ secrets.NEXT_PUBLIC_CLOUDFLARE_SITE_KEY }}
        run: |
          set -e
          TOK='${{ secrets.GHCR_TOKEN }}'
          USR='${{ secrets.GHCR_USERNAME }}'

          if [ -z "$TOK" ] || [ -z "$USR" ]; then
            echo "Error: GHCR_TOKEN and GHCR_USERNAME secrets must be set"
            exit 1
          fi

          # Use BUILD_ID from previous step or timestamp as cache buster
          CACHE_BUST="${DOCKER_BUILD_ID:-$(date +%s)}"
          echo "Using CACHE_BUST=${CACHE_BUST} to force fresh artifact extraction"

          success=0
          for i in 1 2 3; do
            echo "Attempt $i: Logging in to GHCR..."
            echo "$TOK" | docker login ${{ env.REGISTRY }} -u "$USR" --password-stdin

            # Build for linux/amd64 only (most EC2 instances) for faster builds (~3-4 min vs 8-12 min)
            # Change to linux/arm64 if using ARM-based EC2 instances (Graviton)
            if docker buildx build \
              --platform linux/amd64 \
              --build-arg NEXT_PUBLIC_CLOUDFLARE_SITE_KEY=${NEXT_PUBLIC_CLOUDFLARE_SITE_KEY} \
              --build-arg CACHE_BUST=${CACHE_BUST} \
              --provenance=false \
              --sbom=false \
              --cache-from type=local,src=/tmp/.buildx-cache \
              --cache-from type=registry,ref=${{ env.WEBSITE_IMAGE }}:buildcache \
              --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max \
              --cache-to type=registry,ref=${{ env.WEBSITE_IMAGE }}:buildcache,mode=max \
              -t ${{ env.WEBSITE_IMAGE }}:latest \
              -f Dockerfile \
              --push .; then
              echo "Website image buildx push succeeded"
              success=1
              # Move cache to avoid growing cache indefinitely
              rm -rf /tmp/.buildx-cache
              mv /tmp/.buildx-cache-new /tmp/.buildx-cache
              break
            fi

            echo "Attempt $i failed; retrying in $((i*10))s..."
            sleep $((i*10))
          done

          if [ "$success" != 1 ]; then
            echo "Website image buildx push failed after 3 attempts"
            exit 1
          fi

  build-nginx:
    name: Build nginx image (with retry)
    runs-on: ubuntu-latest
    needs: download-build
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Log in to container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.GHCR_USERNAME || github.actor }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Setup Docker layer caching
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          install: true
          driver-opts: |
            network=host
            image=moby/buildkit:latest
          buildkitd-flags: --allow-insecure-entitlement network.host --allow-insecure-entitlement security.insecure

      - name: Build nginx image (single-arch for speed)
        run: |
          set -e
          TOK='${{ secrets.GHCR_TOKEN }}'
          USR='${{ secrets.GHCR_USERNAME }}'

          if [ -z "$TOK" ] || [ -z "$USR" ]; then
            echo "Error: GHCR_TOKEN and GHCR_USERNAME secrets must be set"
            exit 1
          fi

          success=0
          for i in 1 2 3; do
            echo "Attempt $i: Logging in to GHCR..."
            echo "$TOK" | docker login ${{ env.REGISTRY }} -u "$USR" --password-stdin

            # Build for linux/amd64 only for faster builds
            if docker buildx build \
              --platform linux/amd64 \
              --provenance=false \
              --sbom=false \
              --cache-from type=local,src=/tmp/.buildx-cache \
              --cache-from type=registry,ref=${{ env.NGINX_IMAGE }}:buildcache \
              --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max \
              --cache-to type=registry,ref=${{ env.NGINX_IMAGE }}:buildcache,mode=max \
              -t ${{ env.NGINX_IMAGE }}:latest \
              -f nginx/Dockerfile \
              --push .; then
              echo "Nginx image buildx push succeeded"
              success=1
              # Move cache to avoid growing cache indefinitely
              rm -rf /tmp/.buildx-cache
              mv /tmp/.buildx-cache-new /tmp/.buildx-cache
              break
            fi

            echo "Attempt $i failed; retrying in $((i*10))s..."
            sleep $((i*10))
          done

          if [ "$success" != 1 ]; then
            echo "Nginx image buildx push failed after 3 attempts"
            exit 1
          fi

  capture-digests:
    name: Capture image digests after both builds complete
    runs-on: ubuntu-latest
    needs: [build-images, build-nginx]
    steps:
      - name: Log in to container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.GHCR_USERNAME || github.actor }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Wait for registry sync
        run: |
          echo "Waiting for registry to sync both images..."
          echo "This ensures both images are fully propagated before capture"
          sleep 20

      - name: Capture both image digests
        run: |
          set -e

          echo "Fetching website image digest..."
          WEBSITE_DIGEST=$(docker buildx imagetools inspect ${{ env.WEBSITE_IMAGE }}:latest --format '{{json .}}' | jq -r '.manifest.digest')

          echo "Fetching nginx image digest..."
          NGINX_DIGEST=$(docker buildx imagetools inspect ${{ env.NGINX_IMAGE }}:latest --format '{{json .}}' | jq -r '.manifest.digest')

          echo "Website digest: $WEBSITE_DIGEST"
          echo "Nginx digest: $NGINX_DIGEST"

          if [ -z "$WEBSITE_DIGEST" ] || [ "$WEBSITE_DIGEST" = "null" ]; then
            echo "❌ ERROR: Failed to get website digest"
            exit 1
          fi

          if [ -z "$NGINX_DIGEST" ] || [ "$NGINX_DIGEST" = "null" ]; then
            echo "❌ ERROR: Failed to get nginx digest"
            exit 1
          fi

          printf '{"website":"%s","nginx":"%s"}' "$WEBSITE_DIGEST" "$NGINX_DIGEST" > image-digests.json

          echo "✓ Digests captured successfully:"
          cat image-digests.json

          echo ""
          echo "These digests will be used for deployment to ensure"
          echo "containers match the build artifacts synced to S3"

      - name: Upload image digest artifact
        uses: actions/upload-artifact@v4
        with:
          name: image-digests
          path: image-digests.json
          retention-days: 1

  deploy:
    name: Deploy to EC2 via SSH with temporary IP allow
    runs-on: ubuntu-latest
    needs: [capture-digests, sync-cdn]
    permissions:
      contents: read
      id-token: write
    env:
      WEBSITE_IMAGE: ghcr.io/${{ github.repository_owner }}/boudreaux/website
      NGINX_IMAGE: ghcr.io/${{ github.repository_owner }}/boudreaux/nginx
      # Align secret names: ensure AWS_SECURITY_GROUP_ID exists in repo secrets
      AWS_SECURITY_GROUP_ID: ${{ secrets.AWS_SECURITY_GROUP_ID }}
      # Toggle: set to 'true' to fail deploy if self-signed cert remains after issuance attempts
      REQUIRE_TRUSTED_CERT: ${{ vars.REQUIRE_TRUSTED_CERT || 'false' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download image digests artifact
        uses: actions/download-artifact@v4
        with:
          name: image-digests
          path: .

      - name: Export digest environment variables
        shell: bash
        run: |
          WEBSITE_DIGEST=$(jq -r '.website' image-digests.json)
          NGINX_DIGEST=$(jq -r '.nginx' image-digests.json)
          echo "WEBSITE_DIGEST=$WEBSITE_DIGEST" >> $GITHUB_ENV
          echo "NGINX_DIGEST=$NGINX_DIGEST" >> $GITHUB_ENV

          echo "Will deploy with digests:"
          echo "  Website: $WEBSITE_DIGEST"
          echo "  Nginx: $NGINX_DIGEST"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure Security Groups allow HTTP/HTTPS on all attached SGs
        run: |
          set -e

          # Why: EC2 instances can have multiple Security Groups attached.
          # If ANY Security Group lacks HTTP/HTTPS rules, traffic is blocked (Security Groups are stateful AND gates).
          # We auto-discover all SGs attached to the instance and ensure 80/443 are open on each.

          # Discover instance ID by Elastic IP or tag (adjust filter as needed)
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=instance-state-name,Values=running" "Name=tag:Name,Values=*" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text 2>/dev/null || echo "")

          if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" = "None" ]; then
            echo "WARNING: Could not auto-discover instance. Falling back to AWS_SECURITY_GROUP_ID if set."
            if [ -n "${{ env.AWS_SECURITY_GROUP_ID }}" ]; then
              SG_IDS="${{ env.AWS_SECURITY_GROUP_ID }}"
            else
              echo "ERROR: No instance found and AWS_SECURITY_GROUP_ID not set. Cannot ensure Security Group rules."
              exit 1
            fi
          else
            echo "Found instance: $INSTANCE_ID"
            # Get all Security Groups attached to this instance
            SG_IDS=$(aws ec2 describe-instances \
              --instance-ids "$INSTANCE_ID" \
              --query 'Reservations[0].Instances[0].SecurityGroups[*].GroupId' \
              --output text)
          fi

          echo "Security Groups to configure: $SG_IDS"

          # For each Security Group, ensure HTTP/HTTPS are open
          # Why 0.0.0.0/0? This is a public website; visitors come from any IP globally.
          # Port 80: Needed for Let's Encrypt ACME HTTP-01 challenges and HTTP→HTTPS redirect
          # Port 443: HTTPS traffic for the website
          for SG_ID in $SG_IDS; do
            echo "Ensuring ports 80/443 open on $SG_ID..."

            # Add IPv4 rules (idempotent; ignore "already exists" errors)
            aws ec2 authorize-security-group-ingress \
              --group-id "$SG_ID" \
              --ip-permissions IpProtocol=tcp,FromPort=80,ToPort=80,IpRanges='[{CidrIp=0.0.0.0/0,Description="HTTP for web traffic and Let'\''s Encrypt"}]' \
              2>&1 | grep -v "already exists" || true

            aws ec2 authorize-security-group-ingress \
              --group-id "$SG_ID" \
              --ip-permissions IpProtocol=tcp,FromPort=443,ToPort=443,IpRanges='[{CidrIp=0.0.0.0/0,Description="HTTPS for web traffic"}]' \
              2>&1 | grep -v "already exists" || true

            # Optional: Add IPv6 rules (ignore errors if VPC doesn't support IPv6)
            aws ec2 authorize-security-group-ingress \
              --group-id "$SG_ID" \
              --ip-permissions IpProtocol=tcp,FromPort=80,ToPort=80,Ipv6Ranges='[{CidrIpv6=::/0,Description="HTTP IPv6"}]' \
              2>&1 | grep -v -E "already exists|InvalidParameterValue" || true

            aws ec2 authorize-security-group-ingress \
              --group-id "$SG_ID" \
              --ip-permissions IpProtocol=tcp,FromPort=443,ToPort=443,Ipv6Ranges='[{CidrIpv6=::/0,Description="HTTPS IPv6"}]' \
              2>&1 | grep -v -E "already exists|InvalidParameterValue" || true
          done

          echo "Security Group configuration complete."

      - name: Get runner public IP
        id: ip
        uses: haythem/public-ip@v1.3

      - name: Temporarily whitelist runner IP for SSH on all Security Groups
        run: |
          # Why: GitHub Actions runners use dynamic IPs. We temporarily allow SSH from the current runner IP,
          # then revoke it after deployment completes. This must be done on ALL attached Security Groups.

          # Reuse the instance discovery logic
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=instance-state-name,Values=running" "Name=tag:Name,Values=*" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text 2>/dev/null || echo "")

          if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" = "None" ]; then
            # Fallback to env var if discovery fails
            SG_IDS="${{ env.AWS_SECURITY_GROUP_ID }}"
          else
            SG_IDS=$(aws ec2 describe-instances \
              --instance-ids "$INSTANCE_ID" \
              --query 'Reservations[0].Instances[0].SecurityGroups[*].GroupId' \
              --output text)
          fi

          echo "Adding temporary SSH access from ${{ steps.ip.outputs.ipv4 }} to: $SG_IDS"

          for SG_ID in $SG_IDS; do
            aws ec2 authorize-security-group-ingress \
              --group-id "$SG_ID" \
              --protocol tcp \
              --port 22 \
              --cidr "${{ steps.ip.outputs.ipv4 }}/32" \
              2>&1 | grep -v "already exists" || true
          done

      - name: Create .env for remote
        run: |
          cat > .env.deploy << 'EOF'
          NEXT_APP_WEBSITE_IMAGE=${{ env.WEBSITE_IMAGE }}
          NEXT_APP_NGINX_IMAGE=${{ env.NGINX_IMAGE }}
          DATABASE_URL=${{ secrets.DATABASE_URL }}
          AUTH_SECRET=${{ secrets.AUTH_SECRET }}
          AUTH_URL=${{ secrets.AUTH_URL }}
          EMAIL_SERVER_HOST=${{ secrets.EMAIL_SERVER_HOST }}
          EMAIL_SERVER_PORT=${{ secrets.EMAIL_SERVER_PORT }}
          EMAIL_SERVER_USER=${{ secrets.EMAIL_SERVER_USER }}
          EMAIL_SERVER_PASSWORD=${{ secrets.EMAIL_SERVER_PASSWORD }}
          EMAIL_FROM=${{ secrets.EMAIL_FROM }}
          NEXT_PUBLIC_CLOUDFLARE_SITE_KEY=${{ secrets.NEXT_PUBLIC_CLOUDFLARE_SITE_KEY }}
          CLOUDFLARE_SECRET=${{ secrets.CLOUDFLARE_SECRET }}
          EOF

      - name: Upload compose + env
        uses: appleboy/scp-action@v0.1.7
        with:
          # Align secret names: EC2_HOST, EC2_USERNAME, SSH_PRIVATE_KEY
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USERNAME }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          source: docker-compose.prod.yml,.env.deploy
          target: ~/boudreaux

      - name: Deploy via SSH
        uses: appleboy/ssh-action@v1.2.0
        env:
          LETSENCRYPT_EMAIL: ${{ secrets.LETSENCRYPT_EMAIL }}
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USERNAME }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30m
          envs: WEBSITE_DIGEST,NGINX_DIGEST,REQUIRE_TRUSTED_CERT,LETSENCRYPT_EMAIL
          script: |
            set -euo pipefail

            # Ensure project directory exists with correct structure
            mkdir -p ~/boudreaux
            mkdir -p ~/boudreaux/.deploy

            # Check available disk space (fail if less than 2GB free)
            echo "Checking available disk space..."
            AVAILABLE_GB=$(df -BG / | awk 'NR==2 {print $4}' | sed 's/G//')
            echo "Available disk space: ${AVAILABLE_GB}GB"

            if [ "$AVAILABLE_GB" -lt 2 ]; then
              echo "⚠️  WARNING: Low disk space (${AVAILABLE_GB}GB available)"
              echo "Running Docker cleanup to free space..."
              docker system prune -af --volumes || true

              AVAILABLE_GB=$(df -BG / | awk 'NR==2 {print $4}' | sed 's/G//')
              echo "After cleanup: ${AVAILABLE_GB}GB available"

              if [ "$AVAILABLE_GB" -lt 1 ]; then
                echo "❌ ERROR: Insufficient disk space (${AVAILABLE_GB}GB). Need at least 1GB free."
                exit 1
              fi
            else
              echo "✓ Sufficient disk space available"
            fi

            # Create certbot-webroot with sudo if permission denied
            if [ ! -d ~/boudreaux/certbot-webroot ]; then
              sudo mkdir -p ~/boudreaux/certbot-webroot/.well-known/acme-challenge
              sudo chown -R $(whoami):$(id -gn) ~/boudreaux/certbot-webroot
              sudo chmod -R 755 ~/boudreaux/certbot-webroot
            fi

            # Ensure SSH authorized_keys has correct permissions
            mkdir -p ~/.ssh
            chmod 700 ~/.ssh
            if [ -f ~/.ssh/authorized_keys ]; then
              chmod 600 ~/.ssh/authorized_keys
            fi

            cd ~/boudreaux
            TARGET_DIR="$(pwd)"
            LOCAL_USER="$(whoami)"
            LOCAL_GROUP="$(id -gn)"

            # Ensure Docker is installed and running (Ubuntu)
            if ! command -v docker >/dev/null 2>&1; then
              sudo apt-get update
              sudo apt-get remove -y docker.io docker-doc docker-compose podman-docker containerd runc 2>/dev/null || true
              sudo apt-get install -y ca-certificates curl gnupg lsb-release
              if [ ! -f /etc/apt/keyrings/docker.gpg ]; then
                sudo install -m 0755 -d /etc/apt/keyrings
                curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
                sudo chmod a+r /etc/apt/keyrings/docker.gpg
                echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
                sudo apt-get update
              fi
              sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
              sudo systemctl enable docker
              sudo systemctl start docker
            fi

            # Ensure user is in docker group (required for non-root Docker access)
            if ! groups | grep -q docker; then
              echo "Adding $USER to docker group..."
              sudo usermod -aG docker $USER
              echo "User added to docker group. Note: This session will use 'sudo docker' for remaining commands."
              echo "Subsequent deployments will not need sudo after the user logs out and back in."
            fi

            # Check if we can access Docker without sudo (define DOCKER_CMD early)
            if docker ps >/dev/null 2>&1; then
              DOCKER_CMD="docker"
            else
              echo "Using sudo for Docker commands (group membership not yet active in this session)"
              DOCKER_CMD="sudo docker"
            fi

            # Log certificate enforcement setting and current mode
            echo "Certificate enforcement: REQUIRE_TRUSTED_CERT=${REQUIRE_TRUSTED_CERT}"
            if [ -f "$HOME/boudreaux/.selfsigned" ]; then
              echo "Current TLS mode: self-signed (marker present)"
            else
              echo "Current TLS mode: trusted (no self-signed marker)"
            fi

            # Install certbot if not present (one-time setup)
            if ! command -v certbot >/dev/null 2>&1; then
              echo "Installing certbot..."
              sudo apt-get update -qq
              sudo apt-get install -y certbot
            fi

            # Setup Let's Encrypt certificates
            DOMAIN="fakefourrecords.com"
            CERT_PATH="/etc/letsencrypt/live/$DOMAIN"

            # Configure email args for certbot (supports no-email fallback)
            if [ -n "${LETSENCRYPT_EMAIL:-}" ]; then
              CERTBOT_EMAIL_ARGS=(--email "$LETSENCRYPT_EMAIL")
            else
              echo "WARNING: LETSENCRYPT_EMAIL is not set; proceeding without registration email."
              CERTBOT_EMAIL_ARGS=(--register-unsafely-without-email)
            fi

            # Initial standalone issuance (only if Let's Encrypt cert doesn't exist)
            if [ ! -f "$CERT_PATH/fullchain.pem" ]; then
              echo "Obtaining Let's Encrypt certificate for $DOMAIN using standalone mode..."
              # Stop containers that might be using port 80
              $DOCKER_CMD stop nginx website 2>/dev/null || true

              # Request certificate using standalone mode
              sudo certbot certonly --standalone --non-interactive --agree-tos \
                "${CERTBOT_EMAIL_ARGS[@]}" \
                -d "$DOMAIN" -d "www.$DOMAIN" || {
                echo "WARNING: Standalone certificate acquisition failed."
              }
            fi

            # Copy Let's Encrypt cert to deployment directory if available
            if [ -f "$CERT_PATH/fullchain.pem" ]; then
              echo "Installing Let's Encrypt certificate to deployment directory..."
              sudo install -o "$LOCAL_USER" -g "$LOCAL_GROUP" -m 644 "$CERT_PATH/fullchain.pem" "$TARGET_DIR/certificate.pem"
              sudo install -o "$LOCAL_USER" -g "$LOCAL_GROUP" -m 600 "$CERT_PATH/privkey.pem" "$TARGET_DIR/private_key.pem"
              sudo chmod 644 "$TARGET_DIR/certificate.pem"
              sudo chmod 600 "$TARGET_DIR/private_key.pem"

              # Setup automatic renewal cron (idempotent)
              # This ensures certificate auto-renewal even if cert was issued outside of deployment
              echo "Ensuring automatic certificate renewal cron job is configured..."
              if ! sudo crontab -l 2>/dev/null | grep -Fq "certbot renew"; then
                CRON_CMD="0 3 * * * certbot renew --quiet --deploy-hook \"install -o $LOCAL_USER -g $LOCAL_GROUP -m 644 $CERT_PATH/fullchain.pem $TARGET_DIR/certificate.pem && install -o $LOCAL_USER -g $LOCAL_GROUP -m 600 $CERT_PATH/privkey.pem $TARGET_DIR/private_key.pem && chmod 600 $TARGET_DIR/private_key.pem && chmod 644 $TARGET_DIR/certificate.pem && (docker restart nginx || true)\""
                (sudo crontab -l 2>/dev/null; printf '%s\n' "$CRON_CMD") | sudo crontab -
                echo "✓ Cron job added: Certificate will auto-renew daily at 3 AM"
              else
                echo "✓ Cron job already configured"
              fi

              # Remove self-signed marker if present
              if [ -f "$HOME/boudreaux/.selfsigned" ]; then
                rm -f "$HOME/boudreaux/.selfsigned"
                echo "Removed self-signed marker; Let's Encrypt certs are active."
              fi
            fi

            # Fallback: create self-signed cert if no valid cert exists
            if [ ! -f "$HOME/boudreaux/certificate.pem" ] || [ ! -f "$HOME/boudreaux/private_key.pem" ]; then
              echo "Generating temporary self-signed certificate to satisfy compose secrets..."
              # Ensure openssl is available
              if ! command -v openssl >/dev/null 2>&1; then
                sudo apt-get update -qq
                sudo apt-get install -y openssl
              fi
              # Create self-signed cert valid for 30 days
              openssl req -x509 -nodes -newkey rsa:2048 -days 30 \
                -subj "/CN=${DOMAIN}" \
                -keyout "$HOME/boudreaux/private_key.pem" \
                -out "$HOME/boudreaux/certificate.pem"
              sudo chmod 600 "$HOME/boudreaux/private_key.pem" || true
              sudo chmod 644 "$HOME/boudreaux/certificate.pem" || true
              # Mark that we're using a temporary self-signed cert
              touch "$HOME/boudreaux/.selfsigned"
              echo "Self-signed certificate created; will attempt Let's Encrypt issuance after containers start."
            fi

            # Move env into place and secure permissions
            mv -f .env.deploy .env
            chmod 600 .env  # Owner read/write only

            # Login to GHCR using aligned credentials
            echo '${{ secrets.GHCR_TOKEN }}' | $DOCKER_CMD login ghcr.io -u '${{ secrets.GHCR_USERNAME }}' --password-stdin

            WEBSITE_IMAGE='${{ env.WEBSITE_IMAGE }}'
            NGINX_IMAGE='${{ env.NGINX_IMAGE }}'

            # Capture previously deployed digests (for rollback if needed)
            PREV_WEBSITE_DIGEST=$($DOCKER_CMD inspect --format '{{index .RepoDigests 0}}' "$WEBSITE_IMAGE" 2>/dev/null || true)
            PREV_NGINX_DIGEST=$($DOCKER_CMD inspect --format '{{index .RepoDigests 0}}' "$NGINX_IMAGE" 2>/dev/null || true)

            # CRITICAL: Remove old :latest tagged images to prevent Docker from using stale cache
            echo "Removing old :latest images to force fresh pull..."
            $DOCKER_CMD rmi "$WEBSITE_IMAGE:latest" 2>/dev/null || true
            $DOCKER_CMD rmi "$NGINX_IMAGE:latest" 2>/dev/null || true

            # If digests are provided, pin to those; otherwise, use latest and capture digests
            if [ -n "${WEBSITE_DIGEST:-}" ] && [ -n "${NGINX_DIGEST:-}" ]; then
              echo "Using provided digests for deployment"
              echo "  Website: $WEBSITE_DIGEST"
              echo "  Nginx: $NGINX_DIGEST"

              # Pull by fully qualified digest (image@sha256:...)
              $DOCKER_CMD pull "$WEBSITE_IMAGE@$WEBSITE_DIGEST" || {
                echo "❌ Failed to pull website image with digest"
                exit 1
              }
              $DOCKER_CMD pull "$NGINX_IMAGE@$NGINX_DIGEST" || {
                echo "❌ Failed to pull nginx image with digest"
                exit 1
              }

              # Tag with :latest for compose to use
              $DOCKER_CMD tag "$WEBSITE_IMAGE@$WEBSITE_DIGEST" "$WEBSITE_IMAGE:latest"
              $DOCKER_CMD tag "$NGINX_IMAGE@$NGINX_DIGEST" "$NGINX_IMAGE:latest"
              NEW_WEBSITE_DIGEST="$WEBSITE_IMAGE@$WEBSITE_DIGEST"
              NEW_NGINX_DIGEST="$NGINX_IMAGE@$NGINX_DIGEST"
            else
              echo "No digests provided; using latest tags"
              $DOCKER_CMD pull "$WEBSITE_IMAGE:latest" || true
              $DOCKER_CMD pull "$NGINX_IMAGE:latest" || true
              NEW_WEBSITE_DIGEST=$($DOCKER_CMD inspect --format '{{index .RepoDigests 0}}' "$WEBSITE_IMAGE:latest" 2>/dev/null || true)
              NEW_NGINX_DIGEST=$($DOCKER_CMD inspect --format '{{index .RepoDigests 0}}' "$NGINX_IMAGE:latest" 2>/dev/null || true)
            fi

            # Restart stack (force remove any conflicting containers)
            echo "Stopping existing containers..."
            $DOCKER_CMD compose -f docker-compose.prod.yml down --remove-orphans || true

            # Remove any containers with conflicting names
            echo "Removing old containers..."
            $DOCKER_CMD rm -f website nginx 2>/dev/null || true

            # Verify images are fresh (should match digests we just pulled)
            echo "Verifying fresh images..."
            CURRENT_WEBSITE_ID=$($DOCKER_CMD images "$WEBSITE_IMAGE:latest" --format "{{.ID}}")
            CURRENT_NGINX_ID=$($DOCKER_CMD images "$NGINX_IMAGE:latest" --format "{{.ID}}")
            echo "  Website image ID: $CURRENT_WEBSITE_ID"
            echo "  Nginx image ID: $CURRENT_NGINX_ID"

            echo "Starting containers with fresh images..."
            $DOCKER_CMD compose -f docker-compose.prod.yml up -d --force-recreate

            # Verify containers are running
            echo "Verifying containers started successfully..."
            sleep 5
            RUNNING_CONTAINERS=$($DOCKER_CMD ps --format '{{.Names}}' | grep -E 'website|nginx' | wc -l)
            if [ "$RUNNING_CONTAINERS" -lt 2 ]; then
              echo "ERROR: Expected 2 containers (website, nginx) but found $RUNNING_CONTAINERS"
              $DOCKER_CMD ps -a
              $DOCKER_CMD compose -f docker-compose.prod.yml logs --tail=50
              exit 1
            fi
            echo "✓ Both containers are running"

            # Verify containers are healthy before proceeding
            echo "Verifying container health..."
            CONTAINER_COUNT=$($DOCKER_CMD ps --filter "name=website" --filter "name=nginx" --format '{{.Names}}' | wc -l)
            if [ "$CONTAINER_COUNT" -lt 2 ]; then
              echo "❌ ERROR: Expected 2 containers but found $CONTAINER_COUNT"
              $DOCKER_CMD ps -a
              exit 1
            fi
            echo "✓ Both containers running"

            # Give containers time to fully start serving content
            sleep 5

            # Basic health check (allow insecure only if using a temporary self-signed cert)
            echo "Running health check..."
            CURL_OPTS="-fsS --max-time 10"
            if [ -f "$HOME/boudreaux/.selfsigned" ]; then
              echo "Self-signed cert in use; allowing insecure TLS for health check"
              CURL_OPTS="-k $CURL_OPTS"
            fi
            if ! curl $CURL_OPTS https://fakefourrecords.com/api/health; then
              echo 'Health check failed - attempting rollback'

              # Roll back to previous digests if we have them
              if [ -n "$PREV_WEBSITE_DIGEST" ]; then
                # Ensure previous digest is present and retag as :latest
                $DOCKER_CMD pull "$PREV_WEBSITE_DIGEST" || true
                $DOCKER_CMD tag "$PREV_WEBSITE_DIGEST" "$WEBSITE_IMAGE:latest" || true
              fi
              if [ -n "$PREV_NGINX_DIGEST" ]; then
                $DOCKER_CMD pull "$PREV_NGINX_DIGEST" || true
                $DOCKER_CMD tag "$PREV_NGINX_DIGEST" "$NGINX_IMAGE:latest" || true
              fi

              $DOCKER_CMD compose -f docker-compose.prod.yml up -d --force-recreate
              sleep 5
              if ! curl -fsS --max-time 10 https://fakefourrecords.com/api/health; then
                echo 'Rollback health check failed - manual intervention required'
                exit 1
              else
                echo 'Rollback successful'
              fi
            fi

            # Attempt Let's Encrypt webroot issuance if using self-signed cert
            if [ -f "$HOME/boudreaux/.selfsigned" ]; then
              echo "Self-signed certificate detected; attempting Let's Encrypt issuance via webroot with retries..."

              # Pre-flight checks before issuance
              echo "=== Pre-flight checks ==="

              # Check DNS resolution
              RESOLVED_IP=$(dig +short "$DOMAIN" | head -n1 || true)
              if [ -z "$RESOLVED_IP" ]; then
                echo "WARNING: DNS resolution failed for $DOMAIN"
              else
                echo "DNS: $DOMAIN resolves to $RESOLVED_IP"
              fi

              # Create a test challenge file and verify it is served over HTTP
              TEST_TOKEN="deploy-$(date +%s)"
              echo "$TEST_TOKEN" > "$HOME/boudreaux/certbot-webroot/.well-known/acme-challenge/_test"
              if curl -s --max-time 5 "http://$DOMAIN/.well-known/acme-challenge/_test" | grep -q "$TEST_TOKEN"; then
                echo "Port 80: Accessible (HTTP responded)"
              else
                echo "WARNING: Port 80 may be blocked or nginx not serving challenges"
                echo "Ensure Security Group allows inbound TCP 80 from 0.0.0.0/0"
              fi

              # Check if webroot directory exists and is writable
              if [ -d "$HOME/boudreaux/certbot-webroot/.well-known/acme-challenge" ]; then
                echo "Webroot: Challenge directory exists"
                if [ -w "$HOME/boudreaux/certbot-webroot/.well-known/acme-challenge" ]; then
                  echo "Webroot: Directory is writable"
                else
                  echo "WARNING: Webroot challenge directory is not writable"
                fi
              else
                echo "ERROR: Webroot challenge directory should have been created by deploy setup"
              fi

              echo "==========================="

              attempts=0
              max_attempts=3
              backoff_base=30
              while [ $attempts -lt $max_attempts ]; do
                echo "Issuance attempt $((attempts+1)) of $max_attempts"
                sudo certbot certonly --webroot -w "$HOME/boudreaux/certbot-webroot" \
                  --non-interactive --agree-tos \
                  "${CERTBOT_EMAIL_ARGS[@]}" \
                  -d "$DOMAIN" -d "www.$DOMAIN" && break || {
                    echo "Issuance attempt $((attempts+1)) failed"
                  }
                attempts=$((attempts+1))
                if [ $attempts -lt $max_attempts ]; then
                  sleep_duration=$((backoff_base * attempts))
                  echo "Sleeping $sleep_duration seconds before retry..."
                  sleep $sleep_duration
                fi
              done

              if [ -f "$CERT_PATH/fullchain.pem" ]; then
                echo "Webroot issuance succeeded; activating Let's Encrypt certificate."
                sudo install -o "$LOCAL_USER" -g "$LOCAL_GROUP" -m 644 "$CERT_PATH/fullchain.pem" "$HOME/boudreaux/certificate.pem"
                sudo install -o "$LOCAL_USER" -g "$LOCAL_GROUP" -m 600 "$CERT_PATH/privkey.pem" "$HOME/boudreaux/private_key.pem"
                sudo chmod 644 "$HOME/boudreaux/certificate.pem"
                sudo chmod 600 "$HOME/boudreaux/private_key.pem"
                rm -f "$HOME/boudreaux/.selfsigned" || true

                # Setup automatic certificate renewal cron job (runs daily at 3 AM)
                # This is idempotent - will only add the cron job if it doesn't exist
                echo "Setting up automatic certificate renewal cron job..."
                if ! sudo crontab -l 2>/dev/null | grep -Fq "certbot renew"; then
                  CRON_CMD="0 3 * * * certbot renew --quiet --deploy-hook \"install -o $LOCAL_USER -g $LOCAL_GROUP -m 644 $CERT_PATH/fullchain.pem $HOME/boudreaux/certificate.pem && install -o $LOCAL_USER -g $LOCAL_GROUP -m 600 $CERT_PATH/privkey.pem $HOME/boudreaux/private_key.pem && chmod 600 $HOME/boudreaux/private_key.pem && chmod 644 $HOME/boudreaux/certificate.pem && (docker restart nginx || true)\""
                  (sudo crontab -l 2>/dev/null; printf '%s\n' "$CRON_CMD") | sudo crontab -
                  echo "✓ Cron job added: Certificate will auto-renew daily at 3 AM"
                else
                  echo "✓ Cron job already exists"
                fi

                # Restart nginx to pick up new secrets
                $DOCKER_CMD restart nginx || true
              else
                echo "Let's Encrypt certificate still not present after $max_attempts attempts; retaining self-signed cert."
              fi
            fi

            # Secondary certificate chain verification (does not block deploy)
            echo "Verifying presented TLS certificate chain..."
            if command -v openssl >/dev/null 2>&1; then
              CERT_INFO=$(echo | openssl s_client -connect fakefourrecords.com:443 -servername fakefourrecords.com -showcerts 2>/dev/null | openssl x509 -noout -issuer -subject || true)
              if [ -n "$CERT_INFO" ]; then
                echo "--- Certificate Info ---"
                echo "$CERT_INFO"
                SELF_SIGNED=0
                if echo "$CERT_INFO" | grep -qi "issuer=.*fakefourrecords.com"; then
                  echo "NOTE: Self-signed certificate still in use (issuer matches domain)."
                  SELF_SIGNED=1
                fi
                if [ "$SELF_SIGNED" -eq 1 ] && [ "$REQUIRE_TRUSTED_CERT" = "true" ]; then
                  echo "REQUIRE_TRUSTED_CERT=true and self-signed certificate detected; failing deploy."
                  exit 1
                fi
              else
                echo "WARNING: Unable to retrieve certificate info via openssl."
              fi
            else
              echo "openssl not available; skipping certificate chain verification."
            fi

            # Persist current digests on host for audit/rollback reference
            cat > .deploy/digests.json <<JSON
            {
              "website": {
                "prev": "${PREV_WEBSITE_DIGEST}",
                "current": "${NEW_WEBSITE_DIGEST}"
              },
              "nginx": {
                "prev": "${PREV_NGINX_DIGEST}",
                "current": "${NEW_NGINX_DIGEST}"
              }
            }
            JSON

            $DOCKER_CMD system prune -f || true

      - name: Invalidate CloudFront cache for deployed assets
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          CLOUDFRONT_DISTRIBUTION_ID: ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}
        run: |
          # Invalidate all assets to ensure CloudFront serves fresh content from S3
          # This prevents 403 errors from stale cache entries pointing to deleted/moved files
          echo "Invalidating CloudFront cache for all deployed assets..."

          if [ -z "$CLOUDFRONT_DISTRIBUTION_ID" ]; then
            echo "WARNING: CLOUDFRONT_DISTRIBUTION_ID not set, skipping cache invalidation"
            exit 0
          fi

          # Invalidate everything under /media/* (includes _next/static/*)
          # This is necessary because Next.js generates new hashed filenames on each build
          # and CloudFront may cache references to old files that no longer exist
          aws cloudfront create-invalidation \
            --distribution-id "$CLOUDFRONT_DISTRIBUTION_ID" \
            --paths "/media/*" \
            --query 'Invalidation.{Status:Status,Id:Id}' \
            --output table

          echo "✓ CloudFront cache invalidation initiated for all CDN assets"
          echo "Note: Invalidation completes in 1-5 minutes. During this time, some cached assets may still be served."
          echo ""
          echo "✅ Deployment complete! New containers are running with fresh assets in S3."

      - name: Revoke runner IP from all Security Groups
        if: always()
        run: |
          # Cleanup: Remove the temporary SSH rule we added at the start of deployment.
          # Must revoke from ALL Security Groups to avoid leaving stale rules.

          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=instance-state-name,Values=running" "Name=tag:Name,Values=*" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text 2>/dev/null || echo "")

          if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" = "None" ]; then
            SG_IDS="${{ env.AWS_SECURITY_GROUP_ID }}"
          else
            SG_IDS=$(aws ec2 describe-instances \
              --instance-ids "$INSTANCE_ID" \
              --query 'Reservations[0].Instances[0].SecurityGroups[*].GroupId' \
              --output text)
          fi

          echo "Revoking SSH access from ${{ steps.ip.outputs.ipv4 }} on: $SG_IDS"

          for SG_ID in $SG_IDS; do
            aws ec2 revoke-security-group-ingress \
              --group-id "$SG_ID" \
              --protocol tcp \
              --port 22 \
              --cidr "${{ steps.ip.outputs.ipv4 }}/32" \
              2>&1 | grep -v "does not exist" || true
          done
